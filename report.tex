\documentclass[11pt,a4paper]{article}

\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{cite}
\geometry{margin=1in}

\title{Report on \\ \textbf{Online Learning for Min Sum Set Cover and Pandora's Box}}
\author{
    Kasionis Ioannis \\ University of Piraeus \\ \texttt{ioannis.kasionis@gmail.com}
    \and
    Triantafyllos Petros \\ University of Piraeus \\ \texttt{petrostriantafyllos@outlook.com}
}
\date{January 2025}

\begin{document}

\maketitle

\begin{abstract}
This report presents an analysis and summary of the paper \textit{Online Learning for Min Sum Set Cover and Pandora's Box} which provides a simple but effective skeleton for crafting online leaning algorithms for the \textit{Pandora's Box Problem}, the \textit{Min Sum Set Cover Problem (MSSC)}, and other related problems in the field of \textit{Stochastic optimization} \cite{gergatsouli2022online}. Traditionally, these problems assume prior knowledge of value distributions. However, in this work, we explore an online learning approach where we are presented with information over multiple rounds. \par 
Furthermore, the research, extends this framework to a bandit setting where only the values of the opened boxes are shown to the learner after each round.This extention also encompasses other alternatives of the above-mentioned problems \cite{gergatsouli2022online}.
\end{abstract}
\pagebreak

\section{Introduction}
The two essential problems in \textit{Stochastic Optimization} are the \textit{Pandora's Box}, and \textit{MSSC} problems.
Introduced, by Weitzman \cite{weitzman1978optimal} the \textit{Pandora's Box} problem offers a general framework for optimizing decisions under uncertainty, thus the goal is to minimize the cost of uncovering information.
\textit{MSSC}, a variation of \textit{Pandora's box}, deals with scenarios where values are either $0$ or $\infty$ \cite{feige2004approximating}. \par
While the traditional formulation of these problems assumes access to known stochastic distributions \cite{gergatsouli2022online}, this study extends these frameworks to \textit{online} settings, where data arrives sequentially without prior distributional knowledge. \par
The \textit{online learning} perspective brings forth several challenges and opportunities. In this model, adversarial instances or independent scenarios are presented iteratively, and the algorithm must decide on the next action based on the current state and previous observations. The performance metric typically revolves around \textit{regret}, measuring the algorithm's cumulative cost relative to an optimal offline policy \cite{shalev2012online}.
\newline

The work's primary contributions are as follows:
\begin{enumerate}
    \item Propose a computationally efficient online algorithm that achieves constant-competitive guarantees for both MSSC and Pandora's Box generalizations in full-information settings. This includes problems with complex constraints such as matroid bases \cite{bansal2010constant}.
    \item Extend the analysis to \textit{bandit settings}, where only partial feedback about the explored options is available \cite{flaxman2004online}. Despite these limited observations, our algorithm maintains a logarithmic regret bound, providing robust performance guarantees.
    \item Leveraging convex relaxations and online convex optimization techniques, demonstrate practical approximation strategies that bridge theoretical insights with computational efficiency \cite{shalev2007primal}.
\end{enumerate}

The implications of these results are twofold. First, they broaden the applicability of MSSC and Pandora's Box to dynamic, real-world contexts where decisions must adapt to unfolding events. Second, the integration of online convex optimization frameworks highlights the role of mathematical tools in addressing combinatorial challenges \cite{gergatsouli2022online}.

The remainder of the paper is organized as follows. Section 2 presents the problem formulations and related work, Section 3 details the proposed methods, and Sections 4 and 5 provide theoretical analyses and experimental evaluations, respectively. It concludes with discussions on potential future extensions.
\pagebreak

\section{Problem Definitions}
\subsection{Pandora’s Box Problem}
The Pandora’s Box problem involves a set of \( n \) boxes, each containing an unknown value and an associated cost to open. The objective is to determine an order in which to open the boxes to minimize the combined cost of inspection and the value of the selected item. Specifically, given a distribution of potential values and costs for each box, the goal is to identify an optimal strategy that balances exploration (opening boxes) and exploitation (choosing the best available value). 

In the online variant considered in this work, values and costs for the boxes are revealed adversarially over \( T \) rounds, requiring the learner to adopt strategies that adapt to newly presented information while minimizing regret against an optimal offline policy.

\subsection{Min Sum Set Cover (MSSC)}
The MSSC problem is a special case of Pandora’s Box where the values (or costs) inside the boxes are restricted to either \( 0 \) or \( \infty \). The MSSC is particularly relevant in applications where binary outcomes dictate whether an item satisfies certain criteria.
\pagebreak


\section{Methodologies and Results}
The authors developed a three-step framework inspired by the scenario-aware relaxations in \cite{CGT+20}, but adapted to an \emph{online} setting. This framework is applied to two benchmarks---\emph{non-adaptive (NA)} and \emph{partially adaptive (PA)}---under both full-information and bandit feedback.

\subsection*{Scenario-Aware Relaxation and Rounding}
First, they formulate each round’s problem (selecting one box, $k$ boxes, or a matroid basis) as a \emph{scenario-aware} convex program whose fractional solution captures the idea of ``opening'' or ``selecting'' boxes. Building on \cite{CGT+20}, this relaxation is crafted so that a single rounding algorithm can be applied \emph{irrespective of the scenario} revealed. In particular, an $\alpha$-approximate rounding ensures that converting fractional solutions to integral ones (i.e., actual permutations or subsets of boxes) inflates cost by at most a constant factor $\alpha$. This idea eliminates the need to re-optimize for each scenario and is key to preserving no-regret guarantees.

\subsection*{Online Convex Optimization for Full Information vs.\ PA}
When facing a \emph{partially adaptive} benchmark in a \emph{full-information} setting, the authors adopt \emph{Follow the Regularized Leader (FTRL)}. At each round, since the cost function for the entire convex relaxation is observed, they update their fractional decision by minimizing the sum of past losses plus a strongly convex regularizer. Replacing the gradient-descent approach of earlier works with FTRL allows tighter regret bounds. Crucially, once a fractional solution is chosen, it is rounded using the scenario-independent $\alpha$-approximation procedure. In the special case of MSSC, this method recovers the \emph{tight 4-approximation} known offline, showing that their approach effectively matches best-known guarantees while handling adversarial input sequences.

Moreover, against a partially adaptive benchmark, these methods yield \emph{$\alpha$-approximate no-regret} for Pandora’s Box–type objectives under diverse constraints. The paper reports:
\[
\begin{cases}
\text{9.22-approx.\ no-regret for choosing 1 box},\\
O(1)\text{-approx.\ no-regret for choosing $k$ boxes},\\
O(\log k)\text{-approx.\ no-regret for choosing a matroid basis}.
\end{cases}
\]
These results hold in the full-information model, demonstrating constant or logarithmic blow-ups relative to an optimal partially adaptive strategy in hindsight.

\subsection*{Bandit vs.\ PA via Mixing FTRL and ``Open-All''}
Under \emph{bandit feedback}, the learner typically only sees $f^{(t)}(x_t)$ at the chosen point $x_t$, and thus must estimate losses without observing the full function each round. To compensate for missing information, the authors' algorithm randomly alternates between:
\begin{enumerate}
  \item \emph{Opening all $n$ boxes} (paying an upfront cost) to learn the entire cost function for that round.
  \item \emph{Applying an OCO step} (similar to FTRL) using only partial information from the boxes actually opened in the other rounds.
\end{enumerate}
This mixed strategy balances exploration and exploitation and preserves the same rounding-based \(\alpha\)-approximation factor, ensuring \emph{sublinear} regret compared to a PA benchmark. Intuitively, if we never open all boxes, we risk poor gradient estimates; if we always open them, we overspend on inspection. Balancing these extremes provides robust performance with constant or $O(\log k)$ approximate ratios mirroring the full-information case. Specifically the same as in the previous full information case.

\subsection*{Bandit vs.\ NA and the Role of Non-Lipschitz Costs}
A \emph{non-adaptive} benchmark fixes one subset of boxes throughout all rounds. The cost function here is often \emph{non-Lipschitz}, preventing direct application of FTRL. Instead, the paper introduces an \emph{explore--exploit} procedure in which:
\begin{enumerate}
\item \textbf{Explore:} The algorithm fully opens boxes in certain rounds, discovering a new constraint (e.g., a threshold or cost range).  It adds this to a global linear program and uses an ellipsoid subroutine to find a feasible fractional solution.
\item \textbf{Exploit:} That fractional solution is then rounded into a non-adaptive set of boxes, used until the next exploration.
\end{enumerate}
\textbf{Theorem~5.3} states that if there exists a partially adaptive algorithm that is $\beta$-competitive against the NA optimal policy for a given problem (single box, $k$ boxes, or matroid), then the authors’ bandit algorithm matches that $\beta$ up to a small constant factor plus an $o(T)$ regret term. By leveraging previously established constants for these problems, they obtain approximate no-regret bounds of:
\[
\begin{cases}
3.16\text{-approx.\ no regret for choosing 1 box,}\\
12.64\text{-approx.\ no regret for choosing $k$ boxes,}\\
O(\log k)\text{-approx.\ no regret for choosing a matroid basis.}
\end{cases}
\]
Hence, even in the more restrictive NA setting and under partial feedback, the approach still achieves near-optimal competitive guarantees.

\subsection*{Conclusions}
Altogether, this methodology---scenario-aware convex relaxations, online convex optimization to handle partial or full feedback, and scenario-independent rounding---provides unified \emph{constant-competitive no-regret} algorithms, even under adversarial instantiations of Pandora’s Box and MSSC. While the core scenario-aware approach is similar across PA and NA benchmarks, the \emph{constant factors} differ to reflect the more restrictive nature easier to compete against ofnon-adaptive policies. Ultimately, their results show that even in adversarial (bandit) contexts, one can achieve performance close to known offline approximations, including the optimal 4-approximation for MSSC, thus reinforcing the power of integrating convex relaxations, online learning, and robust rounding techniques.

\section{Comparison with Previous Work}
This work builds upon the foundational results in stochastic optimization, particularly in the domains of Pandora’s Box and MSSC. Previous studies have focused on the stochastic case, where values and costs are drawn from known distributions, enabling efficient offline algorithms and partially adaptive strategies \cite{weitzman1978optimal, chawla2020pandora}.

In contrast, our study addresses the online setting where adversarially chosen values and costs are presented over multiple rounds. Notably, our framework simplifies the implementation of competitive algorithms through convex relaxations and online convex optimization techniques, extending prior work by providing guarantees in bandit settings \cite{gergatsouli2022online}. Furthermore, our results generalize MSSC algorithms, achieving constant-factor approximations even under matroid constraints, a challenge that previous work struggled to address effectively \cite{feige2004approximating}.

\section{Applications and Future Work}
The results of this study have practical implications in various domains, including:
\begin{itemize}
    \item \textbf{Resource Allocation:} Optimizing inspection costs in applications such as manufacturing and quality assurance.
    \item \textbf{Search Algorithms:} Enhancing strategies for finding optimal solutions under uncertain information.
    \item \textbf{Online Auctions:} Adapting bidding strategies in adversarial settings.
\end{itemize}

Future work could explore extending the framework to dynamic settings where constraints evolve over time or incorporating richer feedback mechanisms in the bandit setting. Additionally, improving the approximation factors under more complex combinatorial constraints remains an open challenge with significant theoretical and practical relevance.
\pagebreak

\section{Conclusion}
This work presented a comprehensive framework for addressing the online versions of Pandora’s Box and Min Sum Set Cover (MSSC), two cornerstone problems in stochastic optimization. By leveraging convex relaxations and online convex optimization techniques, the authors developed efficient algorithms that achieve constant competitiveness under full information and bandit settings. These algorithms extend the capabilities of previous approaches, particularly in handling adversarially chosen inputs and accommodating complex constraints such as matroids \cite{gergatsouli2022online}.

\appendix
\section{Appendix}
Include any additional proofs, explanations, or extended results here.

\bibliographystyle{unsrt}
\bibliography{report}
\end{document}
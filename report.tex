\documentclass[11pt,a4paper]{article}

\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{cite}
\geometry{margin=1in}

\title{Report on \\ \textbf{Online Learning for Min Sum Set Cover and Pandora's Box}}
\author{
    Kasionis Ioannis \\ University of Piraeus \\ \texttt{ioannis.kasionis@gmail.com}
    \and
    Triantafyllos Petros \\ University of Piraeus \\ \texttt{petrostriantafyllos@outlook.com}
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
This report presents an analysis and summary of the paper \textit{Online Learning for Min Sum Set Cover and Pandora's Box} which provides a simple but effective skeleton for crafting online leaning algorithms for the \textit{Pandora's Box Problem}, the \textit{Min Sum Set Cover Problem (MSSC)}, and other related problems in the field of \textit{Stochastic optimization} \cite{gergatsouli2022online}. Traditionally, these problems assume prior knowledge of value distributions. However, in this work, we explore an online learning approach where we are presented with information over multiple rounds. \par 
Furthermore, the research, extends this framework to a bandit setting where only the values of the opened boxes are shown to the learner after each round.This extention also encompasses other alternatives of the above-mentioned problems \cite{gergatsouli2022online}.
\end{abstract}
\pagebreak

\section{Introduction}
The two essential problems in \textit{Stochastic Optimization} are the \textit{Pandora's Box}, and \textit{MSSC} problems.
Introduced, by Weitzman \cite{weitzman1978optimal} the \textit{Pandora's Box} problem offers a general framework for optimizing decisions under uncertainty, thus the goal is to minimize the cost of uncovering information.
\textit{MSSC}, a variation of \textit{Pandora's box}, deals with scenarios where values are either $0$ or $\infty$ \cite{feige2004approximating}. \par
While the traditional formulation of these problems assumes access to known stochastic distributions \cite{gergatsouli2022online}, this study extends these frameworks to \textit{online} settings, where data arrives sequentially without prior distributional knowledge. \par
The \textit{online learning} perspective brings forth several challenges and opportunities. In this model, adversarial instances or independent scenarios are presented iteratively, and the algorithm must decide on the next action based on the current state and previous observations. The performance metric typically revolves around \textit{regret}, measuring the algorithm's cumulative cost relative to an optimal offline policy \cite{shalev2012online}.
\newline

The works primary contributions are as follows:
\begin{enumerate}
    \item Propose a computationally efficient online algorithm that achieves constant-competitive guarantees for both MSSC and generalizations of Pandora's Box in full-information settings. This includes problems with complex constraints such as matroid bases \cite{bansal2010constant}.
    \item Extend the analysis to \textit{bandit settings}, where only partial feedback about the explored options is available \cite{flaxman2004online}. Despite these limited observations, our algorithm maintains a logarithmic regret bound, providing robust performance guarantees.
    \item Leveraging convex relaxations and online convex optimization techniques, demonstrate practical approximation strategies that bridge theoretical insights with computational efficiency \cite{shalev2007primal}.
\end{enumerate}

The implications of these results are twofold. First, they broaden the applicability of MSSC and Pandora's Box to dynamic, real-world contexts where decisions must adapt to unfolding events. Second, the integration of online convex optimization frameworks highlights the role of mathematical tools in addressing combinatorial challenges \cite{gergatsouli2022online}.

The remainder of the paper is organized as follows. Section 2 presents the problem formulations and related work, Section 3 details the proposed methods, and Sections 4 and 5 provide theoretical analyses and experimental evaluations, respectively. It concludes with discussions on potential future extensions.

\section{Problem Definitions}
Define the MSSC and Pandora's Box problems. Include formal problem statements, such as:
\begin{itemize}
    \item \textbf{Pandora's Box Problem:} Present the goal of minimizing the sum of inspection costs and chosen values.
    \item \textbf{Min Sum Set Cover Problem:} Discuss the special case where values are 0 or $\infty$.
\end{itemize}

\section{Methodologies}
Summarize the key approaches used in the paper, including:
\begin{itemize}
    \item Convex relaxations for online learning.
    \item Online convex optimization frameworks.
    \item Rounding techniques for fractional to integral solutions.
\end{itemize}

\section{Results}
Detail the main results:
\begin{itemize}
    \item Competitive ratios achieved for Pandora's Box and MSSC.
    \item Extensions to bandit settings.
    \item Performance under matroid constraints.
\end{itemize}

\section{Comparison with Previous Work}
Discuss how the paper builds upon or improves results from prior studies. Highlight differences in techniques and results.

\section{Applications and Future Work}
Explain potential applications of these results in real-world scenarios. Propose future directions for research based on open questions identified in the paper.

\section{Conclusion}
Provide a concise summary of the main insights gained from the study and their significance in the field of online learning and optimization.

\appendix
\section{Appendix}
Include any additional proofs, explanations, or extended results here.

\bibliographystyle{unsrt}
\bibliography{report}
\end{document}